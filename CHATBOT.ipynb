{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ISLAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ISLAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ISLAM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\tkinter\\__init__.py\", line 1968, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ISLAM\\AppData\\Local\\Temp\\ipykernel_6244\\4274098812.py\", line 121, in get_user_input\n",
      "    bot_response = self.generate_bot_response(user_message)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ISLAM\\AppData\\Local\\Temp\\ipykernel_6244\\4274098812.py\", line 143, in generate_bot_response\n",
      "    return response(user_message)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ISLAM\\AppData\\Local\\Temp\\ipykernel_6244\\4274098812.py\", line 80, in response\n",
      "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ISLAM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ISLAM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ISLAM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1372, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ISLAM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1259, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "                   ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ISLAM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 110, in _analyze\n",
      "    doc = tokenizer(doc)\n",
      "          ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ISLAM\\AppData\\Local\\Temp\\ipykernel_6244\\4274098812.py\", line 41, in LemNormalize\n",
      "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ISLAM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 142, in word_tokenize\n",
      "    sentences = [text] if preserve_line else sent_tokenize(text, language)\n",
      "                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ISLAM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 119, in sent_tokenize\n",
      "    tokenizer = _get_punkt_tokenizer(language)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ISLAM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\__init__.py\", line 105, in _get_punkt_tokenizer\n",
      "    return PunktTokenizer(language)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ISLAM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1744, in __init__\n",
      "    self.load_lang(lang)\n",
      "  File \"c:\\Users\\ISLAM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\tokenize\\punkt.py\", line 1749, in load_lang\n",
      "    lang_dir = find(f\"tokenizers/punkt_tab/{lang}/\")\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ISLAM\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py\", line 579, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\ISLAM/nltk_data'\n",
      "    - 'c:\\\\Users\\\\ISLAM\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ISLAM\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data'\n",
      "    - 'c:\\\\Users\\\\ISLAM\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\ISLAM\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import sympy as sp  # For evaluating mathematical expressions\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download necessary NLTK packages\n",
    "nltk.download('punkt')  # Download 'punkt' tokenizer\n",
    "nltk.download('wordnet')  # Download 'wordnet' for lemmatization\n",
    "\n",
    "# Sample corpus (enhanced)\n",
    "corpus = [\n",
    "    \"Hello, how can I assist you today?\",\n",
    "    \"I am a chatbot created to help you with your queries.\",\n",
    "    \"You can ask me anything about the topics I'm programmed to understand.\",\n",
    "    \"Goodbye! Have a great day!\",\n",
    "    \"I'm sorry, I didn't catch that. Could you please rephrase?\",\n",
    "    \"I can help with basic programming, general queries, and more.\",\n",
    "    \"Tell me more about what you're interested in.\",\n",
    "    \"I can provide information on various subjects, or just have a chat!\"\n",
    "]\n",
    "\n",
    "# Tokenization\n",
    "sent_tokens = corpus  # Using the predefined corpus as sentences\n",
    "\n",
    "# Preprocessing\n",
    "lemmer = WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "# Keyword Matching\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\")\n",
    "GREETING_RESPONSES = [\"Hi!\", \"Hey!\", \"Hello!\", \"Greetings!\", \"Hi there!\"]\n",
    "\n",
    "# Farewell Responses\n",
    "FAREWELL_INPUTS = (\"bye\", \"see you\", \"goodbye\", \"exit\")\n",
    "FAREWELL_RESPONSES = [\"Bye! Take care.\", \"Goodbye!\", \"See you soon!\", \"It was nice talking to you!\"]\n",
    "\n",
    "def greeting(sentence):\n",
    "    \"\"\"If user's input is a greeting, return a greeting response\"\"\"\n",
    "    if any(greeting in sentence.lower() for greeting in GREETING_INPUTS):\n",
    "        return random.choice(GREETING_RESPONSES)\n",
    "\n",
    "def farewell(sentence):\n",
    "    \"\"\"If user's input indicates farewell, return a farewell response\"\"\"\n",
    "    if any(farewell in sentence.lower() for farewell in FAREWELL_INPUTS):\n",
    "        return random.choice(FAREWELL_RESPONSES)\n",
    "\n",
    "def evaluate_math_expression(expression):\n",
    "    \"\"\"Evaluate mathematical expressions\"\"\"\n",
    "    try:\n",
    "        result = sp.sympify(expression)\n",
    "        return f\"The result is: {result}\"\n",
    "    except:\n",
    "        return \"I couldn't understand the math problem.\"\n",
    "\n",
    "def response(user_response):\n",
    "    \"\"\"Generate a response based on the user input\"\"\"\n",
    "    user_response = user_response.lower()\n",
    "    if any(term in user_response for term in [\"solve\", \"calculate\", \"what is\"]):\n",
    "        # Check if the message is a math problem\n",
    "        return evaluate_math_expression(user_response)\n",
    "    \n",
    "    # Process the response based on predefined corpus\n",
    "    chatbot_response = ''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf[:-1])\n",
    "    idx = vals.argsort()[0][-1]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-1]\n",
    "    if req_tfidf == 0:\n",
    "        chatbot_response = \"I'm sorry! I don't understand you.\"\n",
    "    else:\n",
    "        chatbot_response = sent_tokens[idx]\n",
    "    sent_tokens.remove(user_response)  # Remove the user input to avoid repeated responses\n",
    "    return chatbot_response\n",
    "\n",
    "# GUI using tkinter\n",
    "class ChatbotApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Chatbot\")\n",
    "\n",
    "        # Create chat display area\n",
    "        self.chat_area = scrolledtext.ScrolledText(root, wrap=tk.WORD, height=20, width=80, font=(\"Arial\", 12))\n",
    "        self.chat_area.pack(padx=10, pady=10)\n",
    "        self.chat_area.config(state=tk.DISABLED)\n",
    "\n",
    "        # Create input field\n",
    "        self.user_input = tk.Entry(root, width=80, font=(\"Arial\", 12))\n",
    "        self.user_input.pack(padx=10, pady=10)\n",
    "        self.user_input.bind(\"<Return>\", self.get_user_input)\n",
    "\n",
    "        # Send button\n",
    "        self.send_button = tk.Button(root, text=\"Send\", command=self.get_user_input, font=(\"Arial\", 12))\n",
    "        self.send_button.pack(padx=10, pady=10)\n",
    "\n",
    "        # Initialize with a greeting message\n",
    "        self.display_message(\"Hello! How can I help you today? (You can ask about basic programming, math problems, etc.)\", \"bot\")\n",
    "\n",
    "    def get_user_input(self, event=None):\n",
    "        user_message = self.user_input.get().strip()\n",
    "        if user_message:\n",
    "            self.display_message(user_message, \"user\")\n",
    "            self.user_input.delete(0, tk.END)\n",
    "            bot_response = self.generate_bot_response(user_message)\n",
    "            self.display_message(bot_response, \"bot\")\n",
    "\n",
    "    def display_message(self, message, sender):\n",
    "        self.chat_area.config(state=tk.NORMAL)\n",
    "        if sender == \"user\":\n",
    "            self.chat_area.insert(tk.END, f\"You: {message}\\n\")\n",
    "            self.chat_area.tag_add(\"user\", \"end-2l\", \"end-1l\")\n",
    "            self.chat_area.tag_configure(\"user\", justify='right', background='#e0f7fa', foreground='black')\n",
    "        else:\n",
    "            self.chat_area.insert(tk.END, f\"Bot: {message}\\n\")\n",
    "            self.chat_area.tag_add(\"bot\", \"end-2l\", \"end-1l\")\n",
    "            self.chat_area.tag_configure(\"bot\", justify='left', background='#f1f8e9', foreground='black')\n",
    "        self.chat_area.config(state=tk.DISABLED)\n",
    "        self.chat_area.yview(tk.END)\n",
    "\n",
    "    def generate_bot_response(self, user_message):\n",
    "        if farewell(user_message):\n",
    "            return farewell(user_message)\n",
    "        elif greeting(user_message):\n",
    "            return greeting(user_message)\n",
    "        else:\n",
    "            return response(user_message)\n",
    "\n",
    "# Run the application\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = ChatbotApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
